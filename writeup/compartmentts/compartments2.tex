\documentclass{article}

\usepackage{geometry}
\usepackage{xcolor, colortbl}
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, stmaryrd}
\usepackage{subcaption}

\begin{document}

\input{macros.tex}
\input{tagrules.tex}

\section{Introduction}

In this paper we introduce a novel compartmentalization scheme that distinguishes between memory
that is local to compartments, and memory shared between them by memory-safe pointers. Our Tagged
C implementation places fewer requirements on the underlying tagged hardware that similar systems
from the literature. We describe the specification of the policy in terms of an abstract machine
that gives definition to many memory undefined-behaviors if they are internal to a compartment,
allowing it to describe the behavior of the system in the presence of UB more precisely. We then prove
that Tagged C, running our compartmentalization policy, preserves all properties of the abstract
machine.

\subsection{Motivating Example}

Consider a program for firing missiles that listens on standard input for a password,
checks it against the master password in its own memory, and reports whether it was correct.
If correct, it launches missiles with a system call. It also logs that it recieved an attempted
launch before checking, using an off-the-shelf logging library.

Should the logging library have a vulnerability, it might be used to undermine the security
of the whole system. The harm that it can do is not obvious, assuming the basic restriction that
it cannot call {\tt \_\_sys\_fire\_missiles} directly, but in a memory unsafe setting it could
easily overwrite {\tt master\_pwd} to make it match the supplied password, or leak its value to
be used in a future attempt.

\begin{figure}
  \begin{minipage}[t]{0.5\textwidth}
\begin{verbatim}
#def LEN 1 // parameterize pwd length
char* master_pwd;

void listen() {
  char pwd[LEN];
  while (1) {
    gets(pwd);
    // call external log library 
    log("launch attempt");
    if (check_pwd(pwd)) {
      __sys_fire_missiles();
    }
  }
}
\end{verbatim}
  \end{minipage}
  \begin{minipage}[t]{0.49\textwidth}
\begin{verbatim}
void init() {
  master_pwd = malloc(sizeof(int)*LEN);
  for (int i = 0; i < LEN; i++) {
    master_pwd[i] = '0';
  }
}

int check_pwd(int* pwd) {
  for (int i = 0; i < LEN; i++) {
    if (pwd[i] != master_pwd[i])
      return 0; // bad password
  }
  return 1; // report success
}
\end{verbatim}
  \end{minipage}

\caption{Example Password Checker}
\label{fig:exchecker}
\end{figure}

One approach to making the system secure is to compartmentalize it. All of the above code
is gathered in one unit, which we will call \(A\), and kept separate from the logging library,
which belongs to its own compartment, \(B\). We might further keep the standard library in yet
another compartment, \(C\). The compartments' memories are kept disjoint, except that {\tt gets}
must take a pointer to an array that will be accessible to both \(A\) and \(C\).
(The string argument to {\tt log} will also be a shared global, but this is less interesting.)
The special {\tt \_\_sys\_fire\_missiles} function is modeled as an external call,
outside of both compartments.

How do we know when this compartmentalization is successful? For this specific program,
we could try to prove that it satisfies a particular {\em property} describing its dynamic behavior.
In English, that property would be, ``I only fire the missiles after recieving the valid password.''
But we can't prove that without first getting it into a more rigorous form. For that, we
need a {\em trace semantics} that provides a simple description of how compartments talk to
one another in a given execution. This semantics needs enable reasoning about program behavior
in the presence of shared memory.

Our contributions are as follows:
\begin{itemize}
\item A novel compartmentalization policy for Tagged C that supports cross-compartment
  sharing with fewer constraints on available tags than similar systems from the literature.
\item A formal model of C compartmentalization in the form of an abstract machine that
  supports sharing between compartments while keeping their memories isolated by construction.
\item A proof that the compartmentalization policy is safe with respect to the abstract semantics.
\end{itemize}

\subsection{Related Work}

There are many compartmentalization mechanisms in the literature, and several formal
characterizations of compartmentalized systems.

Abate et al. \cite{} characterize compartmentalization in the presence of undefined
behavior, treating UB as equivalent to compromise by an adversary. Their model does
not support shared memory, and their policy places strong constraints on potential
hardware implementations: they assume linear tags, which are unlikely to appear
in realistic implementations.

Building on their work, El-Korashy et al. \cite{} present a compartmentalization
scheme that does support shared memory. They use a much more restrictive specification
than Abate et al.: their source language is fully memory-safe and has no UB. Their
focus is on showing that they can enforce their compartmentalized calling convention
in a memory safe setting. They are not focused on the underlying enforcement hardware,
but we can assume that any hardware implementation would need to be of similar complexity
to others capable of enforcing full memory-safety, e.g. Acevedo de Amorim et al. \cite{}. 

Thibault et al. \cite{} go deeper into their prove effort: they prove safe compilation of a
compartmentalized version of CompCert C down to a compartmentalized assembly language.
In the process they give up sharing. Their treatment of UB is similar to Abate et al.,
but they add new UB in the form of violations of the compartment interfaces.

Compared to these works in total, our model supports cross-compartment sharing while placing
fewer constraints on the hardware than El-Korashy et al. We also support a full C setting,
though instead of a full compilation chain we attach our policy directly to the Tagged C
source semantics. Our abstract machine is also more precise about its treatment of UB than
the others: it gives definition to some UB, and so can be used to reason about the behavior
of compartments that contain it but still display consistent internal behavior.

\section{Abstract Sharing Semantics}

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \[\begin{split}
    m \in & \mathit{mem}_{\mathit{flat}} \\
    \mathit{read}_{\mathit{flat}} \in & \mathit{mem}_{\mathit{flat}} \rightarrow
    \mathbb{Z} \rightharpoonup \mathit{val} \\
    \mathit{write}_{\mathit{flat}} \in & \mathit{mem}_{\mathit{flat}} \rightarrow
    \mathbb{Z} \rightarrow \mathit{val} \rightharpoonup \mathit{mem}_{\mathit{flat}} \\
    \mathit{alloc}_{\mathit{flat}} \in & \mathit{oracle} \rightarrow
    \mathit{mem}_{\mathit{flat}} \rightarrow \mathbb{Z} \rightharpoonup \\
    & (\mathit{oracle} \times \mathbb{Z}
    \times \mathit{mem}_{\mathit{flat}}) \\
    \mathit{free}_{\mathit{flat}} \in & \mathit{oracle} \rightarrow \mathit{mem}_{\mathit{flat}} \rightarrow
    \mathbb{Z} \rightharpoonup \\
    & (\mathit{oracle} \times \mathit{mem}_{\mathit{flat}}) \\
    \end{split}\]

    \caption{Flat Memory Operations}
    \label{fig:flat}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \[\begin{split}
    b \in & \mathit{block} \\
    m \in & \mathit{mem}_{bo} \\
    \mathit{read}_{bo} \in & \mathit{mem}_{bo} \rightarrow \mathit{block} \rightarrow
    \mathbb{Z} \rightharpoonup \mathit{val} \\
    \mathit{write}_{bo} \in & \mathit{mem}_{bo} \rightarrow \mathit{block} \rightarrow
    \mathbb{Z} \rightarrow \mathit{val} \rightharpoonup \mathit{mem}_{bo} \\
    \mathit{alloc}_{bo} \in & \mathit{oracle} \rightarrow \mathit{mem}_{bo} \rightarrow
    \mathbb{Z} \rightharpoonup \\
    & (\mathit{oracle} \times \mathit{block} \times \mathit{mem}_{bo}) \\
    \mathit{free}_{bo} \in & \mathit{mem}_{bo} \rightarrow \mathit{block} \rightarrow
    \mathbb{Z} \rightharpoonup \\
    & (\mathit{oracle} \times \mathit{mem}_{bo}) \\
    \end{split}\]
    
    \caption{Block-Offset Memory Operations}
    \label{fig:blockoff}
  \end{subfigure}

  \begin{subfigure}{1\textwidth}
    \[\alpha \in \mathit{oracle}\]
    \[M \in \mathcal{M} ::= (\mathit{oracle} \times (\mathit{comp} \rightarrow
    \mathit{mem}_{\mathit{flat}}) \times \mathit{mem}_{bo})\]
    
    
    \caption{Memory State}
    \label{fig:integrated}
  \end{subfigure}

  \caption{Memory State}
  \label{fig:memmod}
\end{figure}

In the example above, we discussed the need to reason about the behavior of a compartmentalized
system in terms of whether memory is being shared or is local to a compartment.
We define a C semantics that builds this kind of reasoning into the memory model.
As shown in Figure \ref{fig:memmod}, we separate the world
into compartments, ranged over by \(A\), \(B\), \(C\), etc., each with its own flat memory.
Additionally, there is a shared memory that follows the
rules of a standard block-offset model. Any compartment may allocate objects in its own
local memory with a call to {\tt malloc}, or in the shared memory with a call to a special
{\tt malloc\_share}.

Using a flat memory model inside of compartments gives two benefits. First, it is generally
less expensive to enforce, and therefore can be implemented on tagged hardware that is more
restrictive. Second, it allows the abstract machine to reason about how programs with some
forms of memory UB will behave after compilation, given a particular compiler and allocator.

On a call to {\tt malloc} in compartment \(C\), the allocation oracle \(\alpha\) provides a base
address that doesn't overlap with an allocated object in that memory, and the appropriately-sized
region starting at that base address is allocated in \(M[C]\).
A call to {\tt malloc\_shared}, meanwhile, generates a fresh block identifier \(b\)
and returns the pointer \((b,0)\); however, such a call may fail due to the system as a whole
running out of memory.

Values now include two kinds of pointer: block-offset pairs that access the shared memory,
and pairs of compartment identifiers and integers that access that compartment's memory.

\[v ::= \dots \mid \mathit{sptr} ~ b ~ \mathit{off} \mid \mathit{lptr} ~ \mathit{addr}\]

The latter local pointers are subject to several restrictions, namely that they cannot be
passed or returned across a compartment boundary, nor written to shared memory. Doing so
is undefined behavior. This means that they will only ever be accessible to the compartment
that owns them. \sna{Note: they could alternatively be forbidden from being recieved/read.}

Load and store semantics differ based on which kind of pointer is being accessed.
Shared pointers access their usual memory at the block and offset that they carry,
while local pointers access the memory of the compartment they carry (which will also
always be the active compartment, because they can't escape.)

[TODO: sample rules]

\paragraph{Allocation Axioms}

[TODO: list out the normal Concrete C axioms here]

\paragraph{Allocator Oracles and Realizability}

\sna{Trying something a little different: we define the flat memories as separate, but then
  specify that we're going to restrict ourselves to allocators that we can realize
  in a single memory. Then we use those allocators to index the relation between high and low
  states.}

In these abstract semantics, we quantify over all allocators that satisfy the above axioms.
Many such allocators are not actually realizeable in a given concrete system: most obviously,
because they allow more memory to be allocated than the system has. A related obstacle is that
we cannot realize in a single address space an allocator that would assign the same internal
address. To simplify our model, we will restrict all of our proofs to deal with a specific subset of
allocators that correspond to realistic allocation systems and therefore are guaranteed to be
realizable.

Formally, we assume that such an allocator instantiates a ``live list'' that associates
every allocation---local allocations in every compartment and shared allocations alike---to
a range of addresses that are guaranteed not to overlap.

\paragraph{Abstract Events and Traces}

One advantage of defining a property in terms of this abstract machine is that we have a clear
distinction between loads and stores that represent compartment interactions (and should appear
in a trace) and those that are internal to a compartment and don't matter.

We define the set of trace events as calls, returns, and cross-compartment loads and stores,
as well as the terminal \(\mathit{stuck}\) event representing undefined behavior.
\[\begin{split}
e \in EvA ::= & \mathit{call} ~ f ~ \overline{v} \\
& \mathit{return} ~ v \\
& \mathit{store} ~ b ~ \mathit{off} ~ v \\
& \mathit{load} ~ b ~ \mathit{off} ~ v \\
& \mathit{stuck} \\
\end{split}\]

A trace is a (possibly infinite) sequence of events.
We write that a particular combination of program \(A[B]\) and oracle \(\alpha\),
produces a trace \(t\) as \(\alpha \vdash A[B] \rightsquigarrow t\).

We describe the behavior of a given program via its trace set \(\llbracket A[B] \rrbracket\): 
\[\llbracket A[B] \rrbracket \triangleq \bigcup_{\alpha}
\{ t \mid \alpha \vdash A[B] \rightsquigarrow t \}\]

\begin{comment}
\paragraph{Properties In Safe Semantics}

Now that we have a system of traces, we can attempt to define our trace property.
Formally, a property \(\pi\) is a set of traces. A program is said to {\em satisfy}
a property if all traces in its trace set are also in that property.

For simplicity, in our example, we selected a pretty simple (hashed) password, just
the repetition of {\tt 0x42} for some number of words; in this discussion let {\tt len = 1}.
We can define the property \(\pi_{\mathit{launch\_safe}}\) as the set of all traces
that do not contain \(\mathit{call} ~ \mathtt{\_\_sys\_fire\_missiles}\) and those in which
that event is preceded by \(\mathit{store} ~ b ~ \mathit{ofs} ~ \mathtt{0x42}\)
and \(\mathit{call} ~ \mathtt{check\_pwd} ~ [\mathit{sptr} ~ b ~ \mathit{ofs}]\).
See Figure \ref{fig:ex_traces} for an example of code that might be in \(B\), and the
resulting trace.

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
\begin{verbatim}
\end{verbatim}
    \caption{Log Code}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \[\begin{split}
    A[B] \rightsquigarrow & \ldots  \cdot \\
    &  \cdot \\
    &  \cdot \\
    &  \cdot \\
    &  \cdot \ldots \\
    \end{split}\]
    \caption{Partial Trace}
  \end{subfigure}

\caption{Compartment \(B\)}
\label{fig:ex_traces}
\end{figure}

The example \(B\) compartment in Figure \ref{fig:ex_traces} satisfies the property when linked with
\(A\). But if we don't know much about the code that we link against, we want to know that {\em any}
compartment \(B\) will satisfy the property. This is called {\em robust satisfaction}.
A policy \(\pi\) is {\em robustly satisfied} by a compartment \(A\) if, for all \(B\),
\(\llbracket A[B] \rrbracket \subseteq \pi\). Ideally, we would want to know that \(A\)
robustly satisfies the property.

Writing properties out in the ad-hoc format above is challenging. More user-friendly approaches include
domain-specific languages like linear temporal logic. It's also easy to get them wrong!
Proving or otherwise enforcing a weak property will be little comfort on seeing the missiles fly.
But even without proving or even rigorously defining a specific property of interest,
the abstract semantics matches the intuitive behavior of programs, so as long as the abstract
semantics are reproduced in our concrete implementation, observers examining this
code can reason about its behavior in different scenarios and convince themselves that it is secure.
\end{comment}

\section{Implementing Compartmentalization in Tagged C}

[This is where I would put the description of the policy, but right now I'm not focused on that.]

\section{Proving Correctness}

Our correctness proofs relate the abstract machine defined above to the Tagged C semantics,
instantiated with the policy above. We give Tagged C a trace semantics
as well, with a separate type of traces reflecting the more concrete setting. We will need to
show that we can reproduce any property from the abstract traces semantics in the concrete one;
the remainder of this writeup is dedicated to defining what it means to reproduce a property
in this compartmentalized setting, and describing how we prove it.

\paragraph{Tagged C Events and Traces}

Tagged C does not distinguish between cross-compartment loads and stores and local ones.
So, its trace model must be somewhat different. Specifically, loads and stores now access
concrete addresses, and load and store events are generated on every load and store, not just the
cross-compartment ones.

We define the set of trace events as calls, returns, loads, stores, stuckness/UB, and now an
explicit {\em fail} event representing failstop behavior.
\[\begin{split}
\hat{e} \in EvC ::= & \mathit{call} ~ f ~ \overline{v} \\
& \mathit{return} ~ v \\
& \mathit{store} ~ \mathit{addr} ~ v \\
& \mathit{load} ~ \mathit{addr} ~ v \\
& \mathit{stuck} \\
& \mathit{fail} \\
\end{split}\]

When a Tagged C program under policy \(\rho\) produces a trace \(\hat{t}\), we write it
\(\alpha \vdash A[B] \rightsquigarrow \hat{t}\), and we define \(\llbracket A[B] \rrbracket_\rho\)
similarly to above.

The intuition behind our notion of safe compartmentalization is that for any compartment
\(A\) and any property that it robustly satisfies in the abstract machine, it should also
robustly satisfy that property in Tagged C with our policy. Additionally, we should sanity-check
our semantics by proving forward simulation. In fact, we will prove a bisimulation, from which
both forward simulation and robust property preservation follow naturally. However, the differences
in the events that comprise traces mean that they can't actually be the same traces; we instead
need to describe the relationship between them that we consider to capture an important notion
of sameness.

\paragraph{Relating traces}

The notion of sameness that we choose is: all loads and stores to shared blocks in the abstract
trace have counterparts in the concrete trace, and none of them overlap with any additional
loads or stores that appear in the concrete trace.

We define a relation \(\sim\) indexed by an assignment of blocks to their base addresses and bounds
(the address immediately following their last byte),
\(\Gamma \in \mathit{block} \rightharpoonup (\mathit{int} \times \mathit{int})\), which we write
\(\Gamma \vdash e \sim \hat{e}\). Defined inductively on values:

\begin{minipage}{0.38\textwidth}
  \judgment{\(\Gamma ~ b = (\mathit{base},\mathit{bound})\)}
           {\(\Gamma \vdash \mathit{sptr} ~ b ~ \mathit{off} \sim
             \mathit{long} ~ (\mathit{base}+\mathit{off})\)}
\end{minipage}
\begin{minipage}{0.38\textwidth}
  \judgment{}{\(\Gamma \vdash \mathit{lptr} ~ C ~ \mathit{addr} \sim
    \mathit{long} ~ \mathit{addr}\)}
\end{minipage}
\begin{minipage}{0.23\textwidth}
  \judgment{}{\(\Gamma \vdash v \sim v\)}
\end{minipage}

Which extends naturally to events:

\begin{minipage}{0.45\textwidth}
  \judgmenttwo{\(\Gamma \vdash v_1 \sim \hat{v}_1\)}
              {\(\Gamma \vdash v_2 \sim \hat{v}_2\)}
              {\(\Gamma \vdash \mathit{load} ~ b ~ \mathit{off} ~ v_2 \sim
                \mathit{load} ~ \mathit{addr} ~ \hat{v}_2\)}
\end{minipage}
\begin{minipage}{0.45\textwidth}
  \judgmenttwo{\(\Gamma \vdash (\mathit{sptr} ~ b ~ \mathit{off}) \sim \hat{v}_1\)}
              {\(\Gamma \vdash v_2 \sim \hat{v}_2\)}
              {\(\Gamma \vdash \mathit{store} ~ b ~ \mathit{off} ~ v_2 \sim
                \mathit{store} ~ \mathit{addr} ~ \hat{v}_2\)}
\end{minipage}

\begin{minipage}{0.45\textwidth}
  \judgment{\(\Gamma \vdash \overline{v} \sim \hat{\overline{v}}\)}
           {\(\Gamma \vdash \mathit{call} ~ f ~ \overline{v} \sim \mathit{call} ~ f ~ \hat{\overline{v}}\)}
\end{minipage}
\begin{minipage}{0.45\textwidth}
  \judgment{\(\Gamma \vdash v \sim \hat{v}\)}
           {\(\Gamma \vdash \mathit{return} ~ v \sim \mathit{return} ~ \hat{v}\)}
\end{minipage}

And finally, to traces, via the possible addition of (non-shared) loads and stores. These must
not overlap with any blocks that are mapped in \(\Gamma\). Note that these are coinductive
definitions. Also, the \(\mathit{fail}\) event in Tagged C relates to any abstract trace.

\judgmenttwo{\(\forall ~ b . \Gamma ~ b = (base,bound) \rightarrow \neg (base \leq addr < bound)\)}
            {\(\Gamma \vdash t \sim \hat{t}\)}
            {\(\Gamma \vdash t \sim \mathit{load} ~ \mathit{addr} ~ \hat{v} \cdot \hat{t}\)}

\judgmenttwo{\(\forall ~ b . \Gamma ~ b = (base,bound) \rightarrow \neg (base \leq addr < bound)\)}
            {\(\Gamma \vdash t \sim \hat{t}\)}
            {\(\Gamma \vdash t \sim \mathit{store} ~ \mathit{addr} ~ \hat{v} \cdot \hat{t}\)}

\judgmenttwo{\(\Gamma \vdash v \sim \hat{v}\)}
            {\(\Gamma \vdash t \sim \hat{t}\)}
            {\(\Gamma \vdash v \cdot t \sim \hat{v} \cdot \hat{t}\)}

\judgment{}
         {\(\Gamma \vdash t \sim \mathit{fail}\)}
            
Which finally brings us to defining the overall trace relation \(\approx\)
by quantifying over \(\Gamma\).

\[t \approx \hat{t} \triangleq \exists \Gamma . \Gamma \vdash t \sim \hat{t}\]

In short, \(\hat{t}\) is a plausible result of starting with \(t\), assigning concrete
addresses to blocks, and recording additional internal memory accesses, possibly failing prematurely.
Conversely, \(t\) is the result of stripping internal loads and stores out of \(\hat{t}\) and
mapping shared ones to their block-offset addresses.

\paragraph{Trace-relating Correctness}

In order for the abstract machine to be at all useful in reasoning about the behavior of Tagged
C, we need to show that Tagged C (with a compartmentalization policy) actually implements it!
This is akin to {\em forward correctness}, albeit with an identity compiler and a target machine
that might failstop. 
Formally, Tagged C with policy \(\rho\) enjoys forward correctness if,
for all programs \(P\) and realizable allocator \(\alpha\), if \(\alpha \vdash P \rightsquigarrow t\),
then there exists some \(\hat{t}\) such that \(t \approx \hat{t}\) and
\(\alpha \vdash P \rightsquigarrow_\rho \hat{t}\).

The possibility of failstop makes this form of correctness quite weak. It might be desirable to
prove that it only failstops under certain conditions, particularly that a fully memory-safe
program never failstops. \sna{I'm considering whether I can just match stuck events in the
  source with failstops in the target, so we only failstop on abstract UB.}

\paragraph{Trace-relating Property Preservation}

Abstract and concrete properties are related if, for every trace in one, the other produces a
related trace.
\[\begin{split}
\pi \approx \hat{\pi} \triangleq
& (\forall t . ~ t \in \pi \Rightarrow \exists \hat{t} . ~ \hat{t} \in \hat{\pi} \land t \approx \hat{t}) \land \\
& (\forall \hat{t} . ~ \hat{t} \in \hat{\pi} \Rightarrow \exists t . ~ t \in \pi \land t \approx \hat{t)} \\
\end{split}\]

A policy \(\rho\) enjoys {\em trace-relating robust property preservation} with respect to the
abstract machine if, for all \(A\) and all \(\pi\) robustly satisfied by \(A\), the concrete
machine with policy \(\rho\) robustly satisfies any \(\hat{\pi}\) when \(\pi \approx \hat{\pi}\).

\[\begin{split}
\mathit{RPP} \triangleq & \forall A ~ \pi ~ \hat{\pi} . ~ \pi \approx \hat{\pi} \Rightarrow \\
& (\forall ~ B . ~ \llbracket A[B] \rrbracket \subseteq \pi) \Rightarrow \\
& (\forall ~ B . ~ \llbracket A[B] \rrbracket_\rho \subseteq \hat{\pi}) \\
\end{split}\]

\paragraph{Proving both Correctness and Property-Preservation Using Bisimulation}

We define the bisimulation relation \(\mathfrak{R}\) between states of the abstract machine and
states of Tagged C, indexed by an allocator \(\alpha\). We'll define the specific relation
below. We first prove that, for any machine states \(S\) and
\(\hat{S}\), if \(S \mathfrak{R}_\alpha \hat{S}\), then one of three cases holds:
\begin{itemize}
\item \(S \longrightarrow S'\) and \(\hat{S} \longrightarrow \hat{S}'\), \(S'\) and \(\hat{S}'\)
  both have the same allocator state \(\alpha'\), and \(S' ~ \mathfrak{R}_{\alpha'} ~ \hat{S}'\).
  Memory trace events are either related, or internal.
\item \(S\) and \(\hat{S}\) are both stuck.
\item \(\hat{S}\) does not step due to failstop.
\end{itemize}

Then, by proving that for any program, its initial states in both machines are related
by \(\mathfrak{R}\), we can show by coinduction that every trace produced by one machine
has a related trace produced by the other.


\end{document}
